---
title: "Fitting Linear Models"
format: html
editor: visual
---

## Data

We will use a dataset from the UCI Machine Learning Repository. This data set is about bike sharing rentals and is available at the assignment link. You can learn more about the data here. The data is available at
https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv

The data description describes the following variables:

• Date : day/month/year

• Rented Bike count - Count of bikes rented at each hour

• Hour - Hour of the day

• Temperature-Temperature in Celsius

• Humidity - %

• Windspeed - m/s

• Visibility - 10m

• Dew point temperature - Celsius

• Solar radiation - MJ/m2

• Rainfall - mm

• Snowfall - cm

• Seasons - Winter, Spring, Summer, Autumn

• Holiday - Holiday/No holiday

• Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)

## Reading Data

Before we can work with the data, we need to read it in!

```{r, warning = FALSE}
library(tidyverse)
library(tidymodels)

bike_data <- readr::read_csv("https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv",
                             locale=locale(encoding="latin1"))
bike_data
```

## EDA

**1. We first need to check for missingness in the data.**

```{r}
sum_na <- function(column){
  sum(is.na(column))
}

na_counts <- bike_data |>
  summarize(across(everything(), sum_na))
na_counts
```

From this output, we see that our data is not missing any data and we can proceed ahead.

**2. Check the column types and the values within the columns to make sure they make sense (basic summary stats for numeric columns and check the unique values for the categorical variables).**

We first can examine the unique values for the categorical variables, and see that all of the unique values for the categorical variables make sense.

```{r}
str(bike_data)

unique(bike_data$Seasons)
unique(bike_data$Holiday)
unique(bike_data$`Functioning Day`)
head(unique(bike_data$Date)) #Here Date is in a character format
```

We can then examine the summary stats for our numeric variables.

```{r}
summary(bike_data[sapply(bike_data, is.numeric)])
```

From our investigation on the summaries of the numeric variables, most of them make sense. However, we may want to investigate the Snowfall and Rainfall variables further.

**3. Convert the Date column into an actual date (if need be). Recall the lubridate package.**

We noticed in our unique character values investigation that the Date variable is originally in the "DD/MM/YYYY" format and thus needs to be converted to a date variable with `lubridate::dmy()`.

```{r}
library(lubridate)

bike_data$Date <- dmy(bike_data$Date)
bike_data
```

**4. Turn the character variables (Seasons, Holiday, and Functioning Day) into factors.**

We can then turn the character variables into factors with `as.factor()`.

```{r}
bike_data <- bike_data |>
  mutate(Seasons = as.factor(Seasons),
         Holiday = as.factor(Holiday),
         `Functioning Day` = as.factor(`Functioning Day`))
bike_data
```

**5. Lastly, rename the all the variables to have easy to use names (I use lower snake case but whatever you’d like is fine)**

We notice that some variables have spaces and their units attached to them, so for ease of use we can rename them.

```{r}
bike_data <- bike_data |>
  rename("date" = "Date",
         "bike_count" = "Rented Bike Count",
         "hour" = "Hour",
         "temperature" = "Temperature(°C)",
         "humidity" = "Humidity(%)",
         "wind_speed" = "Wind speed (m/s)",
         "visibility" = "Visibility (10m)",
         "dew_point_temp" = "Dew point temperature(°C)",
         "solar_radiation" = "Solar Radiation (MJ/m2)",
         "rainfall" = "Rainfall(mm)" ,
         "snowfall" = "Snowfall (cm)",
         "seasons" = "Seasons",
         "holiday" = "Holiday",
         "functioning_day" = "Functioning Day")
```

**6. Create summary statistics (especially related to the bike rental count). These should be done across your categorical variables as well. You should notice something about the Functioning Day variable. Subset the data appropriately based on that.**

We first want to create some summary statistics across our categorical variables.

```{r}
# For seasons
bike_data |>
  group_by(seasons) |>
  summarize(across(bike_count, .fns = list("mean" = mean,
                                                "median" = median,
                                                "var" = var,
                                                "sd" = sd,
                                                "IQR" = IQR,
                                                "min" = min,
                                                "max" = max), .names = "{.fn}_{.col}"))
```


```{r}
# For holiday
bike_data |>
  group_by(holiday) |>
  summarize(across(bike_count, .fns = list("mean" = mean,
                                                "median" = median,
                                                "var" = var,
                                                "sd" = sd,
                                                "IQR" = IQR,
                                                "min" = min,
                                                "max" = max), .names = "{.fn}_{.col}"))
```

```{r}
# For functioning day
bike_data |>
  group_by(functioning_day) |>
  summarize(across(bike_count, .fns = list("mean" = mean,
                                                "median" = median,
                                                "var" = var,
                                                "sd" = sd,
                                                "IQR" = IQR,
                                                "min" = min,
                                                "max" = max), .names = "{.fn}_{.col}"))
```

Here we notice something strange with our Functioning Day categorical variable for the `No` level, all of the numerical summaries are zero across this level. So we can subset our data to only include Functioning Day where the level is `Yes`.

```{r}
bike_data <- bike_data |>
  filter(functioning_day == "Yes")
bike_data
```

**7. To simplify our analysis, we’ll summarize across the hours so that each day has one observation associated with it.**

• (I’m using my new names here. Your names may not match and that’s ok!) Let’s group_by()
the date, seasons, and holiday variables.

• Find the sum of the bike_count, rainfall, and snowfall variables

• Find the mean of all the weather related variables.

• This will be our new data that we’ll analyze!

We'll combine this into one data step.


```{r}
library(dplyr)

daily_data <- bike_data |>
  group_by(date, seasons, holiday) |>
  summarize("total_bike_count" = sum(bike_count),
         "total_rainfall" = sum(rainfall),
         "total_snowfall" = sum(snowfall),
         "average_temp" = mean(temperature),
         "average_humidity" = mean(humidity),
         "average_wind_speed" = mean(wind_speed),
         "average_visibility" = mean(visibility),
         "average_dew_point_temp" = mean(dew_point_temp),
         "average_solar_radiation" = mean(solar_radiation))
daily_data
```

**8. Recreate your basic summary stats and then create some plots to explore relationships. Report correlation between your numeric variables as well.**

We first need to recreate our basic summary stats.

```{r}
summary(daily_data)
```

We can then create some plots to explore the relationships in our daily data.

We first can examine a scatterplot between the total bike count per day and the average temperture for that day.

```{r}
library(ggplot2)
scatter <- ggplot(daily_data, aes(x = average_temp, y = total_bike_count)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Total Bike Count vs Average Temperature",
       x = "Average Temperature (in Celsius)",
       y = "Total Bike Count")
scatter
```

We notice that there appears to be a positive relationship between total bike count and the average temperature, which is to be expected considering people would likely be more inclined to rent a bike to ride in warmer weather.

We can also examine the box plots for total bike count based on the season.

```{r}
boxplot <- ggplot(daily_data, aes(x = seasons, y = total_bike_count, color = seasons)) +
  geom_boxplot() +
  labs(title = "Total Bike Count by Season",
       x = "Season",
       y = "Total Bike Count") +
  scale_color_discrete("Season")
boxplot
```

Again through this boxplot we see a similar trend as in our scatterplot, where less people rent a bike in the colder weather (i.e. winter season).

We can also examine the correlation between our numeric variables in the form of a correlation matrix.

```{r}
library(corrplot)
numeric_vars <- daily_data |>
  select(total_bike_count:average_solar_radiation) |>
  as.data.frame()

numeric_vars <- numeric_vars |> select(-date, -seasons)

# Calculate the correlation matrix
cor_matrix <- cor(numeric_vars, use = "complete.obs")

corrplot(cor_matrix, method = "number")
```

From our correlation plot we can confirm that there is a positive linear relationship between the total bike count and the average temperature.

## Split the Data

**Use functions from tidymodels to split the data into a training and test set (75/25 split). Use the strata argument to stratify the split on the seasons variable**

```{r}
library(tidymodels)
set.seed(123)
daily_split <- initial_split(daily_data, prop = 0.75, strata = seasons)
daily_train <- training(daily_split)
daily_test <- testing(daily_split)
```

**On the training set, create a 10 fold CV split**

```{r}
set.seed(123)
daily_10_fold <- vfold_cv(daily_train, 10)
```


## Fitting MLR

First, let’s create some recipes.

For the 1st recipe:

• Let’s ignore the date variable for modeling (so we’ll need to remove that or give it a different ID) but use it to create a weekday/weekend (factor) variable. (See step 2 of the shinymodels tutorial! You can use step_date() then step_mutate() with a factor(if_else(...)) to create the variable. I then had to remove the intermediate variable created.)

• Let’s standardize the numeric variables since their scales are pretty different.

• Let’s create dummy variables for the seasons, holiday, and our new day type variable


```{r}
recipe_1 <- recipe(total_bike_count ~ ., data = daily_train) |>
  update_role(date, new_role = "ID") |>
  step_date(date, features = "dow", label = TRUE) |>
  step_mutate(day_type = factor(if_else(date_dow %in% c(0,6), "Weekend", "Weekday"))) |>
  step_rm(date_dow) |>
  step_normalize(all_numeric(), -all_outcomes()) |>
  step_dummy(seasons, holiday, day_type)
```

For the 2nd recipe:

• Do the same steps as above.

• Add in interactions between seasons and holiday, seasons and temp, temp and rainfall. For the seasons interactions, you can use starts_with() to create the proper interactions.

```{r}
recipe_2 <- recipe(total_bike_count ~ ., data = daily_train) |>
  update_role(date, new_role = "ID") |>
  step_date(date, features = "dow", label = TRUE) |>
  step_mutate(day_type = factor(if_else(date_dow %in% c(0,6), "Weekend", "Weekday"))) |>
  step_rm(date_dow) |>
  step_normalize(all_numeric(), -all_outcomes()) |>
  step_dummy(seasons, holiday, day_type) |>
  step_interact(terms = ~ starts_with("season"):holiday) |>
  step_interact(terms = ~ starts_with("season"):average_temp) |>
  step_interact(terms = ~ average_temp:total_rainfall)
```

For the 3rd recipe:

• Do the same as the 2nd recipe.

• Add in quadratic terms for each numeric predictor

```{r}
recipe_3 <- recipe(total_bike_count ~ ., data = daily_train) |>
  update_role(date, new_role = "ID") |>
  step_date(date, features = "dow", label = TRUE) |>
  step_mutate(day_type = factor(if_else(date_dow %in% c(0,6), "Weekend", "Weekday"))) |>
  step_rm(date_dow) |>
  step_normalize(all_numeric(), -all_outcomes()) |>
  step_dummy(seasons, holiday, day_type) |>
  step_interact(terms = ~ starts_with("season"):holiday) |>
  step_interact(terms = ~ starts_with("season"):average_temp) |>
  step_interact(terms = ~ average_temp:total_rainfall) |>
  step_poly(all_numeric(), -all_outcomes(), degree = 2, prefix = "quad")
```

Now set up our linear model fit to use the “lm” engine.

```{r}
daily_model <- linear_reg() |>
  set_engine("lm")
```

Fit the models using 10 fold CV via fit_resamples() and consider the training set CV error to choose a best model.

```{r}
daily_CV_fits_1 <- workflow() |>
  add_recipe(recipe_1) |>
  add_model(daily_model) |>
  fit_resamples(daily_10_fold) 

daily_CV_fits_1 |>
  collect_metrics()
```

```{r}
daily_CV_fits_2 <- workflow() |>
  add_recipe(recipe_1) |>
  add_model(daily_model) |>
  fit_resamples(daily_10_fold)

daily_CV_fits_2 |>
  collect_metrics()
```

```{r}
daily_CV_fits_3 <- workflow() |>
  add_recipe(recipe_1) |>
  add_model(daily_model) |>
  fit_resamples(daily_10_fold)

daily_CV_fits_3 |>
  collect_metrics()
```


Using your ‘best’ model, fit the model to the entire training data set (use the last_fit() function).

• Compute the RMSE metric on the test set.

```{r}
#Determine best model above based on lowest rmse
best_model_fit <- daily_CV_fits_1 |>
  last_fit(daily_split) |>
  collect_metrics()
```


• Obtain the final model (fit on the entire training set) coefficient table using extract_fit_parsnip()
and tidy().

```{r}
final_fit <- best_model_fit$.workflow[[1]]
final_coefficients <- final_fit |>
  extract_fit_parsnip() |>
  tidy()
final_coefficients
```

